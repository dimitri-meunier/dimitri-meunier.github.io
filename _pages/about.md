---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

Hi, welcome to my homepage.

I am a PhD student in the [Gatsby Computational Neuroscience Unit](https://www.ucl.ac.uk/gatsby/study-and-work/phd-programme) at [UCL](https://www.ucl.ac.uk) working with [Arthur Gretton](https://www.gatsby.ucl.ac.uk/~gretton/) and an [ELLIS](https://ellis.eu/phd-postdoc) PhD student.

Previously, I was a research assistant at the [Istituto Italiano di Tecnologia](https://iit.it) in the [Computational Statistics and Machine Learning](https://www.iit.it/research/lines/computational-statistics-and-machine-learning) team in Genoa, working with [Massimiliano Pontil](https://scholar.google.com/citations?user=lcOacs8AAAAJ&hl=en) and [Carlo Ciliberto](https://scholar.google.com/citations?user=XUcUAisAAAAJ&hl=en). From May 2020 to November 2020, I was a (remote) research intern with [Pierre Alquier](https://pierrealquier.github.io) and [Emtiyaz Khan](https://emtiyaz.github.io) in the [Approximate Bayesian Inference Team](https://team-approx-bayes.github.io "ApproxBayesTeam") of the [RIKEN Center for Advanced Intelligence Project](https://aip.riken.jp "RikenAIP") in Tokyo.

I graduated the Master [MVA](http://math.ens-paris-saclay.fr/version-francaise/formations/master-mva/) (Machine Learning and Computer Vision) from [ENS Paris Saclay](https://ens-paris-saclay.fr/en) and obtained the engineering degree of [ENSAE](https://www.ensae.fr/en/) specialising in Statistics. 

<!-- 
Prior to that, I received a BSc in Mathematics from the [Université Paris Dauphine](https://dauphine.psl.eu/en/) and spent a semester at the [University of Honk Kong](https://www.hku.hk). 

At RIKEN I worked on the theoretical aspects of Variational Inference and meta learning. My research interests span optimisation, reproducing kernel hilbert spaces, optimal transport and bayesian inference. I am particularly fond of functional analysis, measure and integration theory and high-dimensional probability.--> 

<!--Submitted preprints
====== -->

Publications
======
<em> Preprints </em>

- **Meunier D.**\*, [Li Z.](https://zhuli-michael.github.io)\*, [Christensen T.](https://tmchristensen.com), [Gretton A.](https://www.gatsby.ucl.ac.uk/~gretton/), Nonparametric Instrumental Regression via Kernel Methods is Minimax Optimal. Available on [arxiv:2411.19653](https://arxiv.org/abs/2411.19653). Work in progress.
- **Meunier D.**\*, [Moulin A.](https://antoine-moulin.github.io), Wornbard J., [Kostic V.](https://vladi-iit.github.io), [Gretton A.](https://www.gatsby.ucl.ac.uk/~gretton/), Demystifying Spectral Feature Learning for Instrumental Variable Regression. Submitted. 
- [Mollenhauer M.](https://scholar.google.de/citations?user=nxIcGXwAAAAJ&hl=en), [Mücke N.](https://nicole-muecke.jimdosite.com), **Meunier D.**\*, [Gretton A.](https://www.gatsby.ucl.ac.uk/~gretton/), Regularized least squares learning with heavy-tailed noise is minimax optimal. Available on [arxiv:2505.14214](https://arxiv.org/abs/2505.14214). Submitted.
<!-- Presented at the NeurIPS 2023 Workshop [Mathematics of Modern Machine Learning (M3L)](https://sites.google.com/view/m3l-2023) --> 

<em> Journal </em>

- **Meunier D.**\*, [Li Z.](https://zhuli-michael.github.io)\*, [Gretton A.](https://www.gatsby.ucl.ac.uk/~gretton/), [Kpotufe S.](http://www.columbia.edu/~skk2175/), Nonlinear Meta-Learning Can Guarantee Faster Rates, 2025. Available on [arxiv:2307.10870](https://arxiv.org/abs/2307.10870). To appear SIAM Journal on Mathematics of Data Science (SIMODS).

- [Li Z.](https://zhuli-michael.github.io)\*, **Meunier D.**\*, [Mollenhauer M.](https://scholar.google.de/citations?user=nxIcGXwAAAAJ&hl=en), [Gretton A.](https://www.gatsby.ucl.ac.uk/~gretton/), [Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized Least-Squares Algorithm](https://www.jmlr.org/papers/v25/23-1663.html), 2024. Journal of Machine Learning Research, 2024, vol. 25, no. 181, pp. 1-51. Available on [arxiv:2312.07186](https://arxiv.org/abs/2312.07186).

- **Meunier D.**, [Alquier P.](https://pierrealquier.github.io/index.html), [Meta-strategy for Learning Tuning Parameters with Guarantees](https://www.mdpi.com/1099-4300/23/10/1257). Entropy, 2021, vol. 23, no. 10, 1257. Part of the special issue on [Approximate Bayesian Inference](https://www.mdpi.com/journal/entropy/special_issues/approx_Bayes_inference). Available on [arXiv:2102.02504](https://arxiv.org/abs/2102.02504). 
<!-- <img src="../images/metagraph.png" width="700"> [Code](../files/supplement.zip)-->

<em> Conference </em>

- [Kim J.](https://sites.google.com/g.ecc.u-tokyo.ac.jp/junokim), **Meunier D.**, [Gretton A.](https://www.gatsby.ucl.ac.uk/~gretton/), [Suzuki T.](https://www.google.com/search?client=safari&rls=en&q=taiji+suzuki&ie=UTF-8&oe=UTF-8), [Li Z.](https://zhuli-michael.github.io), Optimality and Adaptivity of Deep Neural Features for Instrumental Variable Regression. To appear ICLR 2025. Available on [arxiv:2501.04898](https://arxiv.org/abs/2501.04898).


- Bozkurt B., [Deaner B.](https://bendeaner.wordpress.com), **Meunier D.**, [Xu L.](https://www.ly9988.work), [Gretton A.](https://www.gatsby.ucl.ac.uk/~gretton/), Density Ratio-based Proxy Causal Learning Without Density Ratios. To appear AISTATS 2025. Available on [arxiv:2503.08371](https://arxiv.org/abs/2503.08371).

- **Meunier D.**\*, Shen Z.\*, [Mollenhauer M.](https://scholar.google.de/citations?user=nxIcGXwAAAAJ&hl=en), [Gretton A.](https://www.gatsby.ucl.ac.uk/~gretton/), [Li Z.](https://zhuli-michael.github.io), [Optimal Rates for Vector-Valued Spectral Regularization Learning Algorithms](https://papers.nips.cc/paper_files/paper/2024/hash/966bf8492dbaa8376f5e3756c09d7edb-Abstract-Conference.html). NeurIPS 2024. Available on [arxiv:2405.14778](https://arxiv.org/abs/2405.14778).

- [Li Z.](https://zhuli-michael.github.io)\*, **Meunier D.**\*, [Mollenhauer M.](https://scholar.google.de/citations?user=nxIcGXwAAAAJ&hl=en), [Gretton A.](https://www.gatsby.ucl.ac.uk/~gretton/), [Optimal Rates for Regularized Conditional Mean Embedding Learning](https://proceedings.neurips.cc/paper_files/paper/2022/hash/1c71cd4032da425409d8ada8727bad42-Abstract-Conference.html). NeurIPS 2022. Available on [arXiv:2208.01711](https://arxiv.org/abs/2208.01711).

- **Meunier D.**, [Pontil M.](http://www0.cs.ucl.ac.uk/staff/m.pontil/), [Ciliberto C.](https://cciliber.github.io), [Distribution Regression with Sliced Wasserstein Kernels](https://proceedings.mlr.press/v162/meunier22b.html). ICML 2022. Available on [arXiv:2202.03926](https://arxiv.org/abs/2202.03926).


<!--<div class="container">
  <div class="image">
    <img style="float: right;" src="../images/gmm_ex.png" width="350"> 
  </div>
  <div class="text"> -->

<!--      </div>
    </div> 
    
<em> Master's Thesis </em>
- [Meta Learning Meets Variational Inference, Learning Priors with Guarantees.](../files/RikenReport.pdf) (2020)
-->

(* Equal contribution)

Teaching
======
- [Gatsby Bridging Programme](https://www.ucl.ac.uk/gatsby/study-and-work/gatsby-bridging-programme) - 2024 & 2025
- Advanced Topics in Machine Learning, [Kernel Methods](http://www.gatsby.ucl.ac.uk/~gretton/coursefiles/rkhscourse.html) - Computational Statistics and Machine Learning MSc - UCL - Fall 2022 & 2023 with [Arthur Gretton](https://www.gatsby.ucl.ac.uk/~gretton/)
- Introduction to stochastic processes - Graduate (M1) - ENSAE Paris - Fall 2020 with [Nicolas Chopin](https://nchopin.github.io)
- Tutor for first year students in Linear Algebra and Functional Analysis - Université Paris Dauphine - Fall 2017

Education
======
<!--[Curriculum Vitae](../files/MeunierDimitriResume.pdf) -->
- MSc in Statistics & Machine Learning, ENS Paris-Saclay, 2019-2020
- MSc in Statistics & Economics, ENSAE Paris, 2018-2020
- BSc in Mathematics, Université Paris Dauphine, 2014-2018

Reading groups
======
- PIMS online [graduate course](https://kantorovich.org/event/ot-gradient-flows/) on Optimal Transport + Gradient Flows, Fall 2023 
- Organiser of the Machine Learning Journal Club at Gatsby CNU, UCL, 2022-2023
- High-Dimensional Probability: An Introduction with Applications in Data Science, Roman Vershynin - January 2023 - March 2023
- Introductory Functional Analysis with Application, Erwin Kreyszig, June 2022 - December 2022
- Learning Theory from First Principles, Francis Bach, April 2021 - September 2021

Attendance
======
- [AISTATS](https://virtual.aistats.org/Conferences/2025), Phuket, 2025
- [ICLR](https://iclr.cc/Conferences/2025), Singapore, 2025
- [2nd RSS/Turing Workshop on Gradient Flows for Sampling, Inference, and Learning](https://rss.org.uk/training-events/events/events-2025/section-groups/2nd-rss-turing-workshop-on-gradient-flows-for-samp/), The Alan Turing Institute, 2025
- [NeurIPS](https://nips.cc/Conferences/2024), Vancouver, 2024
- [Learning and Optimization in Luminy – LOL](https://conferences.cirm-math.fr/3003.html), CIRM, Luminy,  2024
- [Workshop on Functional Inference and Machine Intelligence - FIMI](https://ismseminar.github.io/fimi2024/), Bristol, UK, 2024
- [Machine Learning Summer School](https://groups.oist.jp/mlss), OIST, Okinawa, 2024
- [Gradient Flows For Sampling, Inference, and Learning](https://rss.org.uk/training-events/events/events-2023/sections/gradient-flows-for-sampling,-inference,-and-learni/#eventoverview), Royal Statistical Society, 2023 
- [Meeting in Mathematical Statistics](https://www.i2m.univ-amu.fr/evenements/meeting-in-mathematical-statistics-2022/), CIRM, Luminy,  2022
- [NeurIPS](https://nips.cc/Conferences/2022), New Orleans, 2022
- [ICML](https://icml.cc/Conferences/2022), Baltimore, 2022
- [COLT](https://learningtheory.org/colt2022/), London, 2022





